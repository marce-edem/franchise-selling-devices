########## PrÃ¡ctica 1 #####  AnÃ¡lisis descriptivo de la actividad que siguen las ventas de dispositivos de una franquicia en EspaÃ±a. ##########

######    0_1   ###  Carga de librerÃ­as. #####

library(readxl)
library(moments)
library(corrplot)
library(ggcorrplot)
library(ggplot2)
library(corrgram)
library(sjmisc)
library(stats)
library(faraway)
library(WRS2)
library(tidyverse)
library(hier.part)
library(lmtest)


######    0_2   ###  Funciones. #####

# EstadÃ­sticos.
varianza <- function (x)   sum((x-mean(x))^2)/(length(x)-1)

# Diagrama de cajas.
boxplot2<-function(x,y)
{
  stats=boxplot.stats(x)$stats
  f=fivenum(x)
  stats2 <- c(f[1],stats,f[5])
  stats3 <- c(f[1],f[5])
  
  boxplot(x,main=y,col="chartreuse")
  abline(h=stats[1],lty=2,col="maroon")
  abline(h=stats[5],lty=2,col="maroon")
  text(rep(1.35,5),stats,labels=c('Bigote Inferior','Primer cuartil','Mediana','Tercer cuartil','Bigote superior'))
  text(rep(.5,7),stats2,labels=round(stats2,digits=4),cex=0.6)
  text(rep(0.75,2),stats3,labels=c('MÃNIMO','MÃXIMO'))
}

# Tabla de correlaciones.
col.corrgram <- function(ncol){   
  colorRampPalette(c("darkgoldenrod4", "burlywood1",
                     "darkkhaki", "darkgreen"))(ncol)}

######    0_3   ###  Carga de datos necesarios con la lectura del fichero Excel.   #####

tabla_pr<-read_excel("C:/Users/Marce/Documents/DataHack/Estadistica/Teoria - Material a Compartir-20191001/04.- Analisis Estadistico de Datos - Practica.xlsx")
class(tabla_pr) # Primero miramos la clase para saber el formato que tiene el dataset. Y confirmamos que es un dataframe.

# Aunque ya sabemos que es un rectangulo, chequeamos las dimensiones para saber exactamente de cuantas filas y columnas se compone.
dim(tabla_pr) # 4,403 filas (las cuales son nuestras observaciones) y 18 columnas (las cuales son nuestras variables).
# Aunque tambiÃ©n se pueden mirar por separado de la siguiente manera:
nrow(tabla_pr)
ncol(tabla_pr)

# Pero, Â¿que hay dentro o como se llaman estas variables?, esto se descubre utilizando names() o str() o glimpse().
names(tabla_pr) # nos dice a travÃ©s de un vector de caracteres, los nombres de nuestras variables.
str(tabla_pr) # Si utilizamos str(), nos encontramos de una manera mÃ¡s concisa toda la informaciÃ³n vista hasta ahora, como
glimpse(tabla_pr) # Otra manera igual de eficiente que la anterior pero usando otra libreria. 
# la clase de la tabla, numero de observaciones y variables, los nombres de las variables, su clase e incluso algunos registros.

# Una vez que conocemos las variables. Les modificamos el nombre para no llevar a equivoco en nuesto anÃ¡lisis ni en futuros modelos.
cnames <- c("num", "pro", "rentabieco", "rentabifin", "endp", "liq", "produc", "ventas", "emp", "coe", "edad", "conce", "numac", "numpa", "numest", "estp", "grupo", "fju")
colnames(tabla_pr) <- cnames

# Ahora nos gustaria averiguar como son nuestros datos, y como no serÃ­a muy prÃ¡ctico observarlos todos de golpe, mostramos un resumen.
head(tabla_pr) # utilizando esta funciÃ³n obtenemos por defecto los 6 primeros datos de cada variable.

# Continuando con nuestro anÃ¡lisis, queremos identificar si en alguna de nuestras variables hay NAs (de "Not Available" o datos no disponibles).
table(is.na(tabla_pr)) # Comprobado, no hay ningÃºn valor inexistente. Todo listo para comenzar.



########    1   ######  AnÃ¡lisis descriptivo.   ##########

######    1_1   ###  DeterminaciÃ³n de los estadÃ­sticos.  #####

## Medidas de posiciÃ³n, dispersiÃ³n y forma. ##
# Estas medidas se utilizan para visualizar y analizar como es nuestra distribuciÃ³n.

sapply(tabla_pr, class) # Esta funciÃ³n nos provee de un resultado diferente para cada variable, dependiendo de su clase.
summary(tabla_pr) # MÃ­nimo, primer cuartil, mediana, tercer cuartil y mÃ¡ximo de cada una de las variables de nuestro dataset.

# Ventas
media_ventas <- mean(tabla_pr$ventas)
moda_ventas <- mode(tabla_pr$ventas)
minimo_ventas <- min(tabla_pr$ventas)
primer_cuartil_ventas <- quantile(tabla_pr$ventas,.25)
mediana_ventas <- median(tabla_pr$ventas)
tercer_cuartil_ventas <- quantile(tabla_pr$ventas,.75)
cuartiles_ventas <- quantile(tabla_pr$ventas, c(.25,.50,.75))
maximo_ventas <- max(tabla_pr$ventas)
ri_ventas <- IQR(tabla_pr$ventas)
fivenum(tabla_pr$ventas)
summary(tabla_pr$ventas)
length_ventas <- length((tabla_pr$ventas))
rango_ventas <- range((tabla_pr$ventas))
varianza_ventas <- varianza(tabla_pr$ventas)
desv_tipica_ventas <- sd(tabla_pr$ventas)
coef_asimetria_ventas <- skewness(tabla_pr$ventas) # El coeficiente de asimetria 2.815738 es positivo, 
# por lo tanto el conjunto de datos para la variable VENTAS presenta un alargamiento hacia la derecha, 
# es decir, la distribuciÃ³n de los datos tiene a la derecha una cola mÃ¡s larga.
coef_curtosis_ventas <- kurtosis(tabla_pr$ventas) # La curtosis 12.11034 es positiva y estÃ¡ muy lejos de 0, 
# por lo tanto el conjunto de datos para la variable VENTAS tiene una distribuciÃ³n leptocÃºrtica
coef_variacion_ventas <- desv_tipica_ventas/media_ventas # El coeficiente de variaciÃ³n 1.681256 es positivo
# La media no es NADA representativa con respecto a los datos, por lo que el conjunto de datos es heterogÃ©neo.
coef_variacion_ventas

# Empleados
media_emp <- mean(tabla_pr$emp)
moda_emp <- mode(tabla_pr$emp)
minimo_emp <- min(tabla_pr$emp)
primer_cuartil_emp <- quantile(tabla_pr$emp,.25)
mediana_emp <- median(tabla_pr$emp)
tercer_cuartil_emp <- quantile(tabla_pr$emp,.75)
cuartiles_emp <- quantile(tabla_pr$emp, c(.25,.50,.75))
maximo_emp <- max(tabla_pr$emp)
ri_emp <- IQR(tabla_pr$emp)
fivenum(tabla_pr$emp)
summary(tabla_pr$emp)
length_emp <- length((tabla_pr$emp))
rango_emp <- range((tabla_pr$emp))
varianza_emp <- varianza(tabla_pr$emp)
desv_tipica_emp <- sd(tabla_pr$emp)
coef_asimetria_emp <- skewness(tabla_pr$emp)# El coeficiente de asimetria 2.603358 es positivo, 
# por lo tanto el conjunto de datos para la variable empleados presenta un alargamiento hacia la derecha, 
# es decir, Ã±la distribuciÃ³n de los datos tiene a la derecha una cola mÃ¡s larga.
coef_curtosis_emp <- kurtosis(tabla_pr$emp)# La curtosis 11.53285 es positiva y estÃ¡ muy lejos de 0, 
# por lo tanto el conjunto de datos para la variable empleados tiene una distribuciÃ³n leptocÃºrtica.
coef_variacion_emp <- desv_tipica_emp/media_emp # El coeficiente de variaciÃ³n 1.379502 es positivo
# La media no es NADA representativa con respecto a los datos, por lo que el conjunto de datos es heterogÃ©neo.

# Productividad
media_produc <- mean(tabla_pr$produc)
moda_produc <- mode(tabla_pr$produc)
minimo_produc <- min(tabla_pr$produc)
primer_cuartil_produc <- quantile(tabla_pr$produc,.25)
mediana_produc <- median(tabla_pr$produc)
tercer_cuartil_produc <- quantile(tabla_pr$produc,.75)
cuartiles_produc <- quantile(tabla_pr$produc, c(.25,.50,.75))
maximo_produc <- max(tabla_pr$produc)
ri_produc <- IQR(tabla_pr$produc)
fivenum(tabla_pr$produc)
summary(tabla_pr$produc)
length_produc <- length((tabla_pr$produc))
rango_produc <- range((tabla_pr$produc))
varianza_produc <- varianza(tabla_pr$produc)
desv_tipica_produc <- sd(tabla_pr$produc)
coef_asimetria_produc <- skewness(tabla_pr$produc)# El coeficiente de asimetria 4.892031 es positivo, 
# por lo tanto el conjunto de datos para la variable productividad presenta un alargamiento hacia la derecha, 
# es decir, la distribuciÃ³n de los datos tiene a la derecha una cola mÃ¡s larga.
coef_curtosis_produc <- kurtosis(tabla_pr$produc)# La curtosis 39.95409 es positiva y estÃ¡ muy lejos de 0, 
# por lo tanto el conjunto de datos para la variable productividad tiene una distribuciÃ³n leptocÃºrtica.
coef_variacion_produc <- desv_tipica_produc/media_produc # El coeficiente de variaciÃ³n 1.244842 es positivo
# La media no es NADA representativa con respecto a los datos, por lo que el conjunto de datos es heterogÃ©neo.


## Filtros y Agregaciones. ComparaciÃ³n de Madrid y Barcelona ##

tabla_madrid <- tabla_pr[tabla_pr$pro=="Madrid", ]
tabla_barcelona <- tabla_pr[tabla_pr$pro=="Barcelona", ]
madrid_barcelona <- rbind(tabla_madrid, tabla_barcelona)

summary(madrid_barcelona)
ventas_barcelona <- tabla_barcelona$ventas
ventas_madrid <- tabla_madrid$ventas
# Madrid
total_media_ventas_madrid <- mean(ventas_madrid)
total_ri_ventas_madrid <- IQR(ventas_madrid)
summary(ventas_madrid)
total_varianza_ventas_madrid <- var(ventas_madrid)
total_desv_tipica_ventas_madrid <- sd(ventas_madrid)
total_coef_asimetria_ventas_madrid <- skewness(ventas_madrid) # El coeficiente de asimetria 2.566494 es positivo.
total_coef_curtosis_ventas_madrid <- kurtosis(ventas_madrid) # La curtosis 9.643746 es positiva.
total_coef_variacion_ventas_madrid <- total_desv_tipica_ventas_madrid/total_media_ventas_madrid # El coeficiente de variaciÃ³n 1.718476 es positivo.
# Barcelona
total_media_ventas_barcelona <- mean(ventas_barcelona)
total_ri_ventas_barcelona <- IQR(ventas_barcelona)
summary(ventas_barcelona)
total_varianza_ventas_barcelona <- var(ventas_barcelona)
total_desv_tipica_ventas_barcelona <- sd(ventas_barcelona)
total_coef_asimetria_ventas_barcelona <- skewness(ventas_barcelona) # El coeficiente de asimetria 2.370075 es positivo.
total_coef_curtosis_ventas_barcelona <- kurtosis(ventas_barcelona) # La curtosis 8.765498 es positiva.
total_coef_variacion_ventas_barcelona <- total_desv_tipica_ventas_barcelona/total_media_ventas_barcelona # El coeficiente de variaciÃ³n 1.58457 es positivo.


######    1_2   ###  GrÃ¡ficos.   #####

# Diagrama de cajas de las variables (Ventas - Empleados - Productividad)
par(mfrow=c(1,3))
boxplot2(tabla_pr$ventas, 'Ventas')
boxplot2(tabla_pr$emp, 'Empleados')
boxplot2(tabla_pr$produc, 'Productividad')

## Madrid - Barcelona.
# Ventas
par(mfrow=c(1,2))
boxplot2(tabla_madrid$ventas, "Ventas Madrid")
boxplot2(tabla_barcelona$ventas, "Ventas Barcelona")
# Empleados
par(mfrow=c(1,2))
boxplot2(tabla_madrid$emp, "Empleados Madrid")
boxplot2(tabla_barcelona$emp, "Empleados Barcelona")
# Productividad
par(mfrow=c(1,2))
boxplot2(tabla_madrid$produc, "Productividad Madrid")
boxplot2(tabla_barcelona$produc, "Productividad Barcelona")



######    1_3   ###  Matriz de correlaciones entre todas las variables.   #####

cor(tabla_pr[-2]) # la correlaciÃ³n es algo dificil de ver, redondeamos
round(cor(tabla_pr[-2]),3) # probamos con varios grÃ¡ficos

M <- cor(tabla_pr[-2])
corrplot(M, method = "pie")


corrgram(tabla_pr, order=TRUE, lower.panel=panel.shade,
        upper.panel=panel.pie, text.panel=panel.txt,
        main="Correlacionnes")


ggcorrplot(cor(tabla_pr[ , c("ventas", "emp", "produc", "coe", "numest", "fju", "edad", "rentabieco")], use="complete.obs"), hc.order=TRUE, type="lower", lab=TRUE)

# Las siguientes correlaciones son las 10 mÃ¡s trascendentes, tanto positivas como negativas,
# y vienen al hilo de lo visto en los grÃ¡ficos y descrito anteriormente.
# ventas-emp    :       0.88930023 
# ventas-coe    :       0.52952942
# emp-numest    :       0.51757886
# edad-fju      :       0.49980030
# emp-coe       :       0.49881043
# endp-rentabieco :    -0.49914580    
# ventas-numest :       0.48658969
# emp-fju       :       0.44297679
# emp-edad      :       0.43166520
# ventas-fju    :       0.41529464

# La correlaciÃ³n mÃ¡s alta, y cercana a 1, se da entre ventas y emp(empleados).
# Y por tanto mÃ¡s adelante, comenzaremos nuestro anÃ¡lisis con modelo de regresiÃ³n lineal entre estas dos variables.



######    1_4   ###  Intervalos para las variables de ventas y empleados.   #####

int_ventas <- (max(tabla_pr$ventas)-min(tabla_pr$ventas))/(sqrt(length(tabla_pr$ventas)))
intervalos_ventas <- cut(tabla_pr$ventas, breaks = seq(as.integer(min(tabla_pr$ventas)),as.integer(max(tabla_pr$ventas)),by=as.integer(int_ventas)))
table(intervalos_ventas)

intervalos_emp <- cut(tabla_pr$emp, breaks = c(1,9,50,249), labels= c("Microempresa", "PequeÃ±a empresa", "Mediana empresa"))
table(intervalos_emp)



######    1_5   ###  Tabla cruzada con distribuciones (tabla de contingencia).   #####

tabla_contingencia <- xtabs(~as.integer(intervalos_ventas)+intervalos_emp,
                            data=tabla_pr)
tabla_contingencia

par(mfrow=c(1,1))
hist(tabla_contingencia, col="Orange", border="cyan", main="Tabla de contingencia")


# Frecuencias esperadas.
chisq.test(xtabs(~intervalos_ventas+intervalos_emp,
                 data=tabla_pr))$expected

# Prueba de independencia Chi-Cuadrado de Pearson para determinar si la cantidad de empleados afecta a las ventas.
# HipÃ³tesis nula: ambas variables son independientes.
chisq.test(xtabs(~intervalos_ventas+intervalos_emp,
                 data=tabla_pr), simulate.p.value = TRUE)
# p-valor=NA: no podemos continuar con el anÃ¡lisis

# Prueba exacta de Fisher.
# HipÃ³tesis nula: las variables son independientes, no existe asociaciÃ³n entre ventas y empleados.
fisher.test(xtabs(~intervalos_ventas+intervalos_emp,
                  data=tabla_pr), simulate.p.value = TRUE)
# p-value = 0.0004998 (menor que 0,05) -> rechazamos la hipÃ³tesis nula: existe relaciÃ³n entre las ventas y el nÃºmero de empleados.



########    2   ######  Modelos.   ##########

# RegresiÃ³n lineal 1_1
regresion_1_1 <- lm(ventas~emp, data=tabla_pr)
summary(regresion_1_1)
# Modelo 1_1 : R->0.7909 | ventas = 441.081 * emp - 557.297

# Aunque el intercepto en el modelo 1_1 resulta significativo al estar su p-valor por debajo de 0.05, 
# intentamos ajustarlo aun mÃ¡s para ver si mejoramos nuestro resultado.
modelo_1_1_final <- update(regresion_1_1, .~.-1)
summary(modelo_1_1_final)
# Modelo final : R^2 ajustado -> 0.8443 |
# Con un 84.43% se puede explicar las ventas con la variable de empleados, SIN el termino independiente

# Nube de puntos y recta de mÃ­nimos cuadrados.
par(mfrow=c(1,1))
plot(tabla_pr$ventas, tabla_pr$emp, xlab="Ventas", ylab="Empleados")
abline(regresion_1_1, col="red")


# RegresiÃ³n lineal 1_2
regresion_1_2 <- lm(emp~produc, data=tabla_pr)
summary(regresion_1_2)
# Modelo 1_2 : R->0.0069 | emp = -0.09256 * produc + 17.838

# RegresiÃ³n lineal 1_3
regresion_1_3 <- lm(ventas~produc, data=tabla_pr)
summary(regresion_1_3)
# Modelo 1_3 : R->0.0063 | ventas = 43.828 * produc + 5937.07

# RegresiÃ³n lineal mÃºltiple m_2_1
regresion_m_2_1 <- lm(ventas~emp+produc, data=tabla_pr)
summary((regresion_m_2_1))
# En modelos multiples se usa el R^2 AJUSTADO para ser mÃ¡s rigurosos
# Con un 81.43% se puede explicarse las ventas con las variables de empleados y productividad CONJUNTAMENTE

# RegresiÃ³n lineal mÃºltiple clÃ¡sico - m_2_2
regresion_m_2_2 <- lm(ventas ~ . -num -rentabifin -liq -grupo -edad -numac -estp, data = tabla_pr[-2])
summary(regresion_m_2_2)
# Con un 82.47% se puede explicar las ventas con las variables de productividad, empleados, rentabieco, endp, coe, conce, numpa y numest CONJUNTAMENTE
# Los asteriscos significan que tienen un valor menor a 0,05 y significa que ambas variables son âsagragadasâ!!


# EstimaciÃ³n relativa de los predictores.

# Medida de la bondad del ajuste con RMSPE (Root-Mean-Square Prediction Error).
relat_imp_RMSPE=hier.part(tabla_pr$ventas, tabla_pr[c(3,5,7,9,10,12,14,15,18)], 
                         family="gaussian", gof="RMSPE", barplot=TRUE)
relat_imp_RMSPE

# Importancias relativas segÃºn RMSPE.
# rentabieco  0.2985053%
# endp        0.4446490%
# produc      2.3332842%
# emp        68.4196174%
# coe        11.4655934%
# conce       0.7552096%
# numpa       2.0750294%
# numest      8.8231543%
# fju         5.3849574%

# El predictor con mayor importancia relativa es empleados, yendo acorde a lo indicado inicialmente sobre 
# las grandes correlaciones existentes entre ventas y esta variable dependiente, y significativa en el modelo.
# A continuaciÃ³n se hacen distintos modelos cogiendo todas las variables por encima del 1%.

# RegresiÃ³n lineal mÃºltiple - m_2_3
regresion_m_2_3 <- lm(ventas ~ emp + coe + numest + fju + produc + numpa, data=tabla_pr)
summary(regresion_m_2_3)
# Modelo m_2_3 : R^2->0.8228


# RegresiÃ³n lineal mÃºltiple - m_2_4
regresion_m_2_4 <- lm(ventas ~ emp * coe * numest * fju * produc * numpa -emp:coe:fju:numpa -emp:fju -coe:numest:fju:numpa 
                      -emp:coe:fju:produc:numpa -coe:numest:fju:produc -numest:fju -coe:fju -numest:numpa -emp:coe -numpa
                      -coe:fju:numpa -coe:numpa -numest:produc -coe:numest:numpa -emp:fju:produc:numpa -numest -emp:numest:fju:produc:numpa
                      -emp:coe:numest:produc:numpa -emp:coe:numest:fju:produc:numpa -emp:coe:numest:fju:numpa -emp:coe:numest:numpa
                      -emp:produc:numpa -fju:numpa,
                     data=tabla_pr)
summary(regresion_m_2_4)
# Este modelo es a lo "mÃ¡ximo" a lo que se ha llegado quitando variables, a partir de aqui el R^2 ajustado empieza a descender
# Modelo m_2_4 : R^2 -> 0.939 |
# Con un 93.9% se puede explicar las ventas con las variables de empleados, productividad, coe, fju,
# emp:numest, coe:numest, emp:produc, coe:produc, fju:produc, emp:numpa, produc:numpa, emp:coe:numest, 
# emp:coe:fju, emp:numest:fju, coe:numest:fju, emp:coe:produc, emp:numest:produc, coe:numest:produc, 
# emp:fju:produc, coe:fju:produc, numest:fju:produc, emp:coe:numpa, emp:numest:numpa, emp:fju:numpa,
# numest:fju:numpa, coe:produc:numpa, numest:produc:numpa, fju:produc:numpa, emp:coe:numest:fju,
# emp:coe:numest:produc, emp:coe:fju:produc, emp:numest:fju:produc, emp:numest:fju:numpa, emp:coe:produc:numpa,
# emp:numest:produc:numpa, coe:numest:produc:numpa, coe:fju:produc:numpa, numest:fju:produc:numpa,
# emp:coe:numest:fju:produc, coe:numest:fju:produc:numpa
# CONJUNTAMENTE

# A su vez podemos reajustar este modelo m_2_4 , pues el intercepto tampoco resulta significativo 
# p-valor = 0.121831 -> >0,05).
modelo_m_2_4_final<-update(regresion_m_2_4, .~.-1)
summary(modelo_m_2_4_final)
# Todas las variables resultan ser significativas en este modelo al estar en todas su p-valor por debajo de 0.05.
# Descartar el intercepto no hace que el modelo deje de ser significativo (p-valor sigue siendo 
# muy cercano a cero), y la bondad del ajuste ahora aumenta notoriamente: R^2 ajustado=0,9549.
# Modelo final m_2_4: R^2 ajustado -> 0.9549 |
# Con un 95.49% puede explicarse las ventas con las variables anteriormente comentadas SIN el termino independiente.



# RegresiÃ³n lineal mÃºltiple - m_2_5
regresion_m_2_5 <- lm(ventas ~ emp * coe * numest * fju * produc -numest:produc -emp:coe:numest -emp:coe:numest:fju:produc -emp:coe:fju:produc 
                   -numest:fju -emp:coe:fju -emp:fju:produc,
                   data=tabla_pr)
summary(regresion_m_2_5)
# Este modelo es a lo "mÃ¡ximo" a lo que se ha llegado quitando variables, a partir de aqui el R^2 ajustado empieza a descender
# Modelo m_2_4 : R^2 -> 0.933 |
# Con un 93.3% se puede explicar las ventas con las variables de empleados, productividad, coe, numest, fju,
# emp:produc, emp:coe, emp:numest, coe:numest, emp:fju, coe:fju, coe:produc, fju:produc, emp:numest:fju, 
# coe:numest:fju, emp:coe:produc, emp:numest:produc, coe:numest:produc, coe:fju:produc, numest:fju:produc, 
# emp:coe:numest:fju, emp:coe:numest:produc, emp:numest:fju:produc, coe:numest:fju:produc CONJUNTAMENTE

# Aunque el intercepto en el modelo m_2_5 resulta significativo al estar su p-valor por debajo de 0.05, 
# intentamos ajustarlo aun mÃ¡s para mejorar nuestro resultado.
modelo_2_5_final <- update(regresion_m_2_5, .~.-1)
summary(modelo_2_5_final)
# Modelo final : R^2 ajustado -> 0.9504 |
# Con un 95.04% puede explicar las ventas con las variables de empleados, productividad, coe, numest, fju,
# emp:produc, emp:coe, emp:numest, coe:numest, emp:fju, coe:fju, coe:produc, fju:produc, emp:numest:fju, 
# coe:numest:fju, emp:coe:produc, emp:numest:produc, coe:numest:produc, coe:fju:produc, numest:fju:produc, 
# emp:coe:numest:fju, emp:coe:numest:produc, emp:numest:fju:produc, coe:numest:fju:produc CONJUNTAMENTE,
# SIN el termino independiente


# A continuaciÃ³n se prueban 2 modelos bajo la recomendaciÃ³n de negocio que podrÃ­an mejorar nuestros resultados:
# RegresiÃ³n lineal mÃºltiple - m_2_6
regresion_m_2_6 <- lm(ventas ~ emp * produc, data=tabla_pr)
summary(regresion_m_2_6)
# Modelo 2_6 : R^2->0.9267 | ventas = 96.8579 * emp -5.4615 * produc + 24.8763 emp:produc -499.7054
# Con un 92.67% se puede explicar las ventas con las variables de empleados, productividad y empleados:productividad CONJUNTAMENTE

# Aunque el intercepto en el modelo m_2_6 resulta significativo al estar su p-valor por debajo de 0.05, intentamos ajustarlo aun mÃ¡s para mejorar nuestro resultado.
modelo_m_2_6_final <- update(regresion_m_2_6, .~.-1)
summary(modelo_m_2_6_final)
# Modelo final : R^2->0.9453 | ventas = 81.7520 * emp -16.2954 * produc + 25.4355 emp:produc
# Con un 94.53% se puede explicar las ventas con las variables de empleados, productividad y empleados:productividad CONJUNTAMENTE, SIN el termino independiente


# RegresiÃ³n lineal mÃºltiple - m_2_7
regresion_m_2_7 <- lm(ventas ~ emp * produc -produc, data=tabla_pr)
summary(regresion_m_2_7)
# Con un 92.66% se puede explicar las ventas con las variables de empleados y empleados:productividad CONJUNTAMENTE
# Quitar solo producitividad no mejora en ningÃºn aspecto el modelo anterior (m_2_6)

## ComparaciÃ³n de los dos modelos.

# ComparaciÃ³n de los modelos con m_2_4(todas las variables relevantes multiplicadas) y m_2_6(ventas*empleados).
anova(regresion_m_2_4, regresion_m_2_6)
# p-valor=2.2*10^-16 < 0.05 -> aÃ±adir mÃ¡s variables y multiplicarlas si supone efectos relevantes.

# ComparaciÃ³n de los modelos m_2_4 y m_2_4_final(sin intercepto)
anova(regresion_m_2_4, modelo_m_2_4_final)
# p-valor=0.1218 > 0,05 -> eliminar el intercepto no supone efectos relevantes en el modelo.

# ComparaciÃ³n de los modelos m_2_6 y m_2_6_final(sin intercepto)
anova(regresion_m_2_6, modelo_m_2_6_final)
# p-valor=2.83*10^-12 > 0,05 -> eliminar el intercepto si supone efectos relevantes en el modelo.

# El modelo final es por tanto: ventas = 76.2221*emp + (-37056.7587*coe) + (-722.4961*fju) + (-17.9292*produc) 
# + (-26.6364*emp*numest) + 20501.8698*coe*numest + 21.8619*emp*produc + 1791.6135*coe*produc + (-51.5836*fju*produc)
# + (-46.1410*emp*numpa) + 142.6817*produc*numpa + 659.1461*emp*coe*numest + 1976.0101*emp*coe*fju + 35.9301*emp*numest*fju 
# + (-15813.1321*coe*numest*fju + 41.1239*emp*coe*produc + 2.8136*emp*numest*produc + (-792.3279*coe*numest*produc) 
# + 6.2822*emp*fju*produc + 1343.1950*coe*fju*produc + 36.5169*numest*fju*produc + (-732.9427*emp*coe*numpa) 
# + 19.8840*emp*numest*numpa + 55.4974*emp*fju*numpa + (-183.3024*numest*fju*numpa) + (-2422.9581*coe*produc*numpa 
# + (-54.2707*numest*produc*numpa) + (-186.3391*fju*produc*numpa) + (-853.9640*emp*coe*numest*fju) 
# + (-62.2667*emp*coe*numest*produc) + (-187.5318*emp*coe*fju*produc) + (-3.9172*emp*numest*fju*produc) 
# + (-18.7871*emp*numest*fju*numpa) + 62.4009*emp*coe*produc*numpa + (-0.3381*emp*numest*produc*numpa) 
# + 829.7561*coe*numest*produc*numpa + 2206.7192*coe*fju*produc*numpa + 79.5384*numest*fju*produc*numpa 
# + 80.5576*emp*coe*numest*fju*produc + (-862.6652*coe*numest*fju*produc*numpa) -1



# Mostramos ademÃ¡s los intervalos de confianza a distintos niveles de significaciÃ³n para los coeficientes estimados:
# Estos se han realizado con el siguiente mÃ¡s simple con mejor resultado, aun asi en estos momentos y con mis conocimientos
# no soy capaz de interpretarlo.
confint(modelo_m_2_4_final)


# VisualizaciÃ³n de la distribuciÃ³n de los datos.
par(cex=.6)
boxplot(ventas~emp*produc, data=tabla_pr, las=2)

# ANOVA clÃ¡sico.
summary(aov(ventas~emp*produc, data=tabla_pr))


### Criterios para seleccionar el mejor modelo.

# Criterio de InformaciÃ³n de Akaike: mediante la funciÃ³n step seleccionaremos aquÃ©l que 
# muestre el AIC con valor mÃ­nimo.

# FunciÃ³n step en su formato hacia adelante.
reg.lm0<-lm(involact~1, chicago_ejercicio)
slm.forward<-step(reg.lm0, scope=~race+fire+theft+age, direction="forward")

slm_forward1 <- step(regresion_m_2_4, scope=modelo_m_2_4_final, direction="forward")
slm_forward2 <- step(regresion_m_2_4, scope=modelo_m_2_4_final, direction="backward")
slm_forward3 <- step(regresion_m_2_4, scope=modelo_m_2_4_final, direction="both")
slm_forward4 <- step(regresion_m_2_4, scope=regresion_m_2_6, direction="both")
slm_forward5 <- step(regresion_m_2_4, scope=regresion_m_2_6, direction="backward")
slm_forward6 <- step(modelo_2_4_final, scope=modelo_m_2_6_final, direction="both")
slm_forward7 <- step(modelo_1_1_final, scope=regresion_1_1, direction="both")
slm_forward8 <- step(modelo_1_1_final, scope=modelo_m_2_6_final, direction="both")
# En estos casos, al ser los valores de AIC de <none> para todas las comparaciones el mÃ¡s bajo 
# entiendo que es mejor no usar ninguna de esas combinaciones. Por otro lado, 
# al tener AIC tan altos y dado que hay tan poca diferencia relativa entre ellos, 
# no podemos usar este criterio.


# DiagnÃ³stico del modelo y presencia de outliers.

# Comenzamos viendo que dependiendo del tramo en el que nos situemos se satisface o no uno de los supuestos del modelo lineal, pues
# solo a partir del valor 20,000 existe independencia de los residuos.

# Prueba de Durbin-Watson. H0: no existe autocorrelaciÃ³n.
dwtest(modelo_m_2_4_final)
# p-valor=0,01704<0,05: Rechazamos HipÃ³tesis nula. 
# Existe autocorrelaciÃ³n. Como hemos comentado los residuos son dependientes en los primeros tramos. 
# No se satisface el supuesto del modelo lineal.

par(mfrow=c(1,1))
plot(modelo_m_2_4_final)

# GrÃ¡fico Residuals -vs- Fitted: residuos frente a valores ajustados o predichos. 

# La variabilidad de los residuos parece aumentar con los valores ajustados, aparentemente no existe azar
# en la dispersiÃ³n del grÃ¡fico, a partir del valor 20,000 aproximadamente como se ha comentado anteriormente.

# Vemos, ademÃ¡s, que aparecen tres outliers: nÃºmeros de registro 3467(Sevilla), 3971(Valencia) y 2603(Madrid).

# GrÃ¡fico Normal Q-Q: GrÃ¡fico cuantil-cuantil normal de los residuos estandarizados. 

# El modelo de regresiÃ³n lineal asume que los errores tienen comportamiento normal, 
# y en consecuencia su representaciÃ³n sigue la lÃ­nea recta que se distingue en el grÃ¡fico.

# Con el test de normalidad de Shapiro-Wilk vemos que los residuos siguen una distribuciÃ³n normal.
# H0: los residuos muestran comportamiento normal.
shapiro.test(resid(modelo_m_2_4_final))
# p-valor=2.2*10^-15<0,05, por lo que rechazamos H0, los residuos NO se comportan como una distribuciÃ³n normal.
# NO cumple el supuesto del modelo lineal.


# GrÃ¡fico Scale-Location.

# Argumento anÃ¡logo al desarrollado en el primero de los grÃ¡ficos, considerando en este caso
# la raÃ­z cuadrada de los residuos estandarizados.

# GrÃ¡fico Residuals -vs- Leverage: distancias de Cook.

# Graficamos los outliers sobre las variables analizadas.
cooks <- cooks.distance(modelo_m_2_4_final)
hat <- lm.influence(modelo_m_2_4_final)$hat

par(mfrow=c(2,1))
plot(modelo_m_2_4_final, col=ifelse(cooks>quantile(cooks,.90),'red','black'),pch=20)
plot(modelo_m_2_4_final, col=ifelse(hat>quantile(hat,.90),'green','black'),pch=20)
attach(tabla_pr)
par(mfrow=c(2,1))
plot(emp+coe+fju+produc+emp*numest+coe*numest+emp*produc+coe*produc+fju*produc+emp*numpa
    +produc*numpa+emp*coe*numest+emp*coe*fju+emp*numest*fju+coe*numest*fju+emp*coe*produc
    +emp*numest*produc+coe*numest*produc+emp*fju*produc+coe*fju*produc+numest*fju*produc
    +emp*coe*numpa+emp*numest*numpa+emp*fju*numpa+numest*fju*numpa+coe*produc*numpa
    +numest*produc*numpa+fju*produc*numpa+emp*coe*numest*fju+emp*coe*numest*produc
    +emp*coe*fju*produc+emp*numest*fju*produc+emp*numest*fju*numpa+emp*coe*produc*numpa
    +emp*numest*produc*numpa+coe*numest*produc*numpa+coe*fju*produc*numpa+numest*fju*produc*numpa
    +emp*coe*numest*fju*produc+coe*numest*fju*produc*numpa-1,predict(modelo_m_2_4_final),
    col=ifelse(cooks>quantile(cooks,.9),'red','black'),pch=20)
plot(emp+coe+fju+produc+emp*numest+coe*numest+emp*produc+coe*produc+fju*produc+emp*numpa
    +produc*numpa+emp*coe*numest+emp*coe*fju+emp*numest*fju+coe*numest*fju+emp*coe*produc
    +emp*numest*produc+coe*numest*produc+emp*fju*produc+coe*fju*produc+numest*fju*produc
    +emp*coe*numpa+emp*numest*numpa+emp*fju*numpa+numest*fju*numpa+coe*produc*numpa
    +numest*produc*numpa+fju*produc*numpa+emp*coe*numest*fju+emp*coe*numest*produc
    +emp*coe*fju*produc+emp*numest*fju*produc+emp*numest*fju*numpa+emp*coe*produc*numpa
    +emp*numest*produc*numpa+coe*numest*produc*numpa+coe*fju*produc*numpa+numest*fju*produc*numpa
    +emp*coe*numest*fju*produc+coe*numest*fju*produc*numpa-1,predict(modelo_m_2_4_final),
    col=ifelse(hat>quantile(hat,.9),'green','black'),pch=20)


# Existen outliers y hay varios que se situan mÃ¡s allÃ¡ de la lÃ­nea que marca la distancia de Cook
# igual a 1, con lo que siendo influyentes SI se puede considerar que su influencia sea alta.

# Con el resultado anterior se antoja recomendable acudir al modelo robusto. 
# El cual no tengo conocimientos suficientes para llevar a cabo.



########    3   ######  Inferencia estadÃ­stica sobre las ventas entre Madrid y Barcelona   ##########

### PreparaciÃ³n de datasets para que las muestras aleatorias tengan la misma longitud ###

summary(madrid_barcelona)
ventas_barcelona <- tabla_barcelona$ventas
ventas_madrid <- tabla_madrid$ventas

muestra_madrid <- sample.int(ventas_madrid, size=250, replace=FALSE)
muestra_media_ventas_madrid <- mean(muestra_madrid)
muestra_ri_ventas_madrid <- IQR(muestra_madrid)
summary(muestra_madrid)
muestra_varianza_ventas_madrid <- var(muestra_madrid)
muestra_desv_tipica_ventas_madrid <- sd(muestra_madrid)
muestra_coef_asimetria_ventas_madrid <- skewness(muestra_madrid) # El coeficiente de asimetria 0.057782 es positivo.
muestra_coef_curtosis_ventas_madrid <- kurtosis(muestra_madrid) # La curtosis 1.814677 es positiva.
muestra_coef_variacion_ventas_madrid <- muestra_desv_tipica_ventas_madrid/muestra_media_ventas_madrid # El coeficiente de variaciÃ³n 0.577808 es positivo.

muestra_barcelona <- sample.int(ventas_barcelona, size=250, replace=FALSE)
muestra_media_ventas_barcelona <- mean(muestra_barcelona)
muestra_ri_ventas_barcelona <- IQR(muestra_barcelona)
summary(muestra_barcelona)
muestra_varianza_ventas_barcelona <- var(muestra_barcelona)
muestra_desv_tipica_ventas_barcelona <- sd(muestra_barcelona)
muestra_coef_asimetria_ventas_barcelona <- skewness(muestra_barcelona) # El coeficiente de asimetria 0.082435 es positivo.
muestra_coef_curtosis_ventas_barcelona <- kurtosis(muestra_barcelona) # La curtosis 1.872939 es positiva.
muestra_coef_variacion_ventas_barcelona <- muestra_desv_tipica_ventas_barcelona/muestra_media_ventas_barcelona # El coeficiente de variaciÃ³n 0.561907 es positivo.

## GrÃ¡ficos ##
par(mfrow=c(1,2))
hist(muestra_madrid, main="Muestra Ventas Madrid")
hist(muestra_barcelona, main="Muestra Ventas Barcelona")


######    3_1   ###  Contraste de HipÃ³tesis.  #####

## HipÃ³tesis nula: la distribuciÃ³n es normal.
shapiro.test(muestra_madrid)
# p-valor<0,05. Rechazamos la hipÃ³tesis nula, la distribuciÃ³n NO es normal.

## HipÃ³tesis nula: la distribuciÃ³n es normal.
shapiro.test(muestra_barcelona)
# p-valor<0,05. Rechazamos la hipÃ³tesis nula, la distribuciÃ³n NO es normal.
# Asumimos que si lo es y continuamos con el anÃ¡lisis.

## H0: las varianzas son iguales.
var.test(muestra_madrid, muestra_barcelona)
# p-valor<0,05. Rechazamos H0, las varianzas NO son iguales.

## H0: las medias son iguales.

# Alternativa 1
t.test(muestra_madrid, muestra_barcelona, var.equal=T)
# p-valor<0,05. Rechazamos H0, las medias NO son iguales.

# Alternativa 2
t.test(ventas_madrid, ventas_barcelona, var.equal=T)
# p-valor>0,05. Aceptamos H0, las medias son iguales con el muestreo inicial.

# H0: la media de X es menor o igual que la media de Y.
# HipÃ³tesis alternativa: mu(x)>mu(y)
t.test(muestra_madrid, muestra_barcelona, var.equal=T, alternative = "greater", paired = T)
# p-valor>0,05. Aceptamos H0, las ventas en Madrid son peores o iguales que las de Barcelona.

